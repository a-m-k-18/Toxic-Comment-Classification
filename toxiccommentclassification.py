# -*- coding: utf-8 -*-
"""ToxicCommentClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YaRhvcUIjPuzKcUhSVsxCVG82NVNgQG7
"""

# Commented out IPython magic to ensure Python compatibility.
import sys, os, re, csv, codecs, numpy as np, pandas as pd
import sklearn
import matplotlib.pyplot as plt
# %matplotlib inline
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('/content/drive/My Drive/train.csv')

test.head()

train.isnull().any(),test.isnull().any()

list_classes = ['toxic' , 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']
y_train = train[list_classes].values
X_train = train['comment_text']

max_features = 20000
tokenizer = Tokenizer(num_words = max_features)
tokenizer.fit_on_texts(list(X_train))
X_tokenized_train = tokenizer.texts_to_sequences(X_train)

tokenizer.word_counts

tokenizer.word_index

"""Lets check out the length of all comments in the given data."""

totalNumWords = [len(comment) for comment in X_tokenized_train]
plt.hist(totalNumWords , bins = np.arange(0,410,10))
plt.show()

"""### We see the most of the comment length lies in between 20-50 words and very few words have more than 150.
So we can make all the comment lengths to 200 by adding 0 to all the comments having less than 200 words and removing few words in those comments which has more than 200 words.
"""

max_len = 200
X_tr = pad_sequences(X_tokenized_train , maxlen = max_len , value = 0.0)
x_train, x_test, y_train, y_test = train_test_split(X_tr, y, 
test_size= 0.1, random_state=0)

"""# BUILDING OUR MODEL"""

inp = Input(shape=(max_len, ))

"""## STEP 1 : EMBEDDING
This step is used to map the words into a defined vector space depending on the distance of the surrounding words in a sentence.

For example take these two sentences :
Sentence 1 : The cat purs.
Sentence 2 : The kitty hunts mice.

When we map it into a vector space the cat and the kitty will have very less between them indicating that they have very similar meaning.It learns via the context given.
"""

embed_size = 128 #Size of the defined vector space
x = Embedding( max_features , embed_size)(inp)

"""## STEP 2 : LSTM/RNN - Long Short Term Memory

The Recurrent Neural Network works by feeding the output of a previous network into the input of current network and we will get the output after 'X' number of iterations.

We can make use of the output from the previous embedding layer which outputs a 3-D tensor of (None, 200, 128) into the LSTM layer. What it does is going through the samples, recursively run the LSTM model for 200 times, passing in the coordinates of the words each time. And because we want the unrolled version, we will receive a Tensor shape of (None, 200, 60), where 60 is the output dimension we have defined.
"""

x = LSTM(60 , return_sequences = True , name = "lstm_layer")(x)

"""## STEP 3 : MAX-POOLING

Now the current 3D Tensor has a shape of (None , 200 , 60) after the LSTM process. We need to convert this 3D tensor into an 2D tensor.  We reshape carefully to avoid throwing away data that is important to us.

We can use differnt methods of pooling but for this data I am going to GlobalMaxPooling.
"""

x = GlobalMaxPool1D()(x)

"""## DROPOUT AND DENSE

With a 2D Tensor in our hands, we pass it to a Dropout layer which indiscriminately "disable" some nodes so that the nodes in the next layer is forced to handle the representation of the missing data and the whole network could result in better generalization.

We connect the output of dropout layer to RELU function , which basically 

**Activation( (Input X Weights) + Bias)**
"""

x = Dropout(0.1)(x)

x = Dense(50 , activation = "relu")(x)

x = Dropout(0.1)(x)

"""Finally, we feed the output into a Sigmoid layer. The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1."""

x = Dense(6 , activation = "sigmoid")(x)

Now we train the model

model = Model(inputs = inp , outputs = x)
model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])

"""We'll feed in a list of 32 padded, indexed sentence for each batch and split 10% of the data as a validation set. This validation set will be used to assess whether the model has overfitted, for each batch."""

batch_size = 32
epochs = 2
model.fit(x_train , y_train , batch_size=batch_size, epochs=epochs, validation_split=0.1)



y_pred = model.predict(x_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred))

